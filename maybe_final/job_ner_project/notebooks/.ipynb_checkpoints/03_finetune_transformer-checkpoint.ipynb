{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355c4a3c-a088-4358-9185-0e586fc9809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 1\n",
    "# Install required libs (run in a notebook cell)\n",
    "!pip install -q transformers datasets seqeval evaluate accelerate\n",
    "!pip install -q tokenizers\n",
    "\n",
    "# If you haven't already:\n",
    "!pip install -q pandas scikit-learn\n",
    "\n",
    "# Then imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663b3c4-f579-4467-a7a2-b78400ee11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 2\n",
    "csv_path = \"../data/annotated/job_ner_annotations_full_20_jds.csv\"  # adjust path if needed\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()\n",
    "\n",
    "# Reconstruct tokens & labels per sentence_id (job id).\n",
    "sentences = []\n",
    "labels = []\n",
    "ids = []\n",
    "\n",
    "for sid, grp in df.groupby(\"sentence_id\", sort=True):\n",
    "    toks = grp[\"token\"].tolist()\n",
    "    labs = grp[\"label\"].tolist()\n",
    "    sentences.append(toks)\n",
    "    labels.append(labs)\n",
    "    ids.append(int(sid))\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92143bf9-073e-40af-ab98-a312991d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 3\n",
    "train_tokens = sentences[:16]\n",
    "train_labels = labels[:16]\n",
    "\n",
    "test_tokens = sentences[16:]\n",
    "test_labels = labels[16:]\n",
    "\n",
    "# Optionally create a small validation from train (e.g., last 2 of train)\n",
    "val_tokens = train_tokens[-2:]\n",
    "val_labels = train_labels[-2:]\n",
    "train_tokens = train_tokens[:-2]\n",
    "train_labels = train_labels[:-2]\n",
    "\n",
    "print(\"Train/Val/Test sizes (sequences):\", len(train_tokens), len(val_tokens), len(test_tokens))\n",
    "\n",
    "# Make HF datasets from lists\n",
    "def build_hf_dataset(token_seqs, label_seqs):\n",
    "    records = []\n",
    "    for toks, labs in zip(token_seqs, label_seqs):\n",
    "        records.append({\"tokens\": toks, \"labels\": labs})\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "hf_train = build_hf_dataset(train_tokens, train_labels)\n",
    "hf_val = build_hf_dataset(val_tokens, val_labels)\n",
    "hf_test = build_hf_dataset(test_tokens, test_labels)\n",
    "\n",
    "dataset = DatasetDict({\"train\": hf_train, \"validation\": hf_val, \"test\": hf_test})\n",
    "dataset\n",
    "\n",
    "# quick check to ensure every sequence has same number of tokens and labels\n",
    "def check_alignment(token_seqs, label_seqs):\n",
    "    bad = []\n",
    "    for i, (t, l) in enumerate(zip(token_seqs, label_seqs)):\n",
    "        if len(t) != len(l):\n",
    "            bad.append((i, len(t), len(l)))\n",
    "    return bad\n",
    "\n",
    "print(\"train alignment issues:\", check_alignment(train_tokens, train_labels))\n",
    "print(\"val alignment issues:\", check_alignment(val_tokens, val_labels))\n",
    "print(\"test alignment issues:\", check_alignment(test_tokens, test_labels))\n",
    "\n",
    "from pprint import pprint\n",
    "pprint({\"tokens\": train_tokens[0], \"labels\": train_labels[0]})\n",
    "\n",
    "\n",
    "#quick verify on HF datasets\n",
    "for split in [\"train\",\"validation\",\"test\"]:\n",
    "    ds = dataset[split]\n",
    "    # check that each row has tokens and labels and lengths match\n",
    "    for i in range(min(3, len(ds))):\n",
    "        t = ds[i][\"tokens\"]; l = ds[i][\"labels\"]\n",
    "        assert len(t) == len(l), f\"Mismatch in {split} row {i}: {len(t)} vs {len(l)}\"\n",
    "print(\"HF datasets built and validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91335f58-1054-4bab-8a9b-b4083d300ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 4\n",
    "# Collect all unique labels in BIO form from dataset\n",
    "unique_labels = sorted({lab for labs in labels for lab in labs})\n",
    "# Ensure 'O' present\n",
    "if \"O\" not in unique_labels:\n",
    "    unique_labels.append(\"O\")\n",
    "unique_labels = sorted(unique_labels, key=lambda x: (x == \"O\", x))  # put O last or first as you like\n",
    "\n",
    "# But for token-classification we need unique entity tag types (B-xxx, I-xxx).\n",
    "label_list = unique_labels\n",
    "label_list\n",
    "# Create maps\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "print(\"Num labels:\", len(label_list))\n",
    "print(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d037cf-e4f9-49de-b22d-0b61c0decfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in a notebook cell\n",
    "!rm -rf ~/.cache/huggingface/transformers/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022527a6-1647-4c29-b405-3495008a7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 5\n",
    "model_checkpoint = \"bert-base-cased\"   # change to 'distilbert-base-cased' or other if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# alignment function\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokens = example[\"tokens\"]\n",
    "    labels = example[\"labels\"]\n",
    "    # join tokens with space for tokenizer but we will use token-level alignment via word_ids\n",
    "    # encode with is_split_into_words=True to preserve original tokenization\n",
    "    tokenized_inputs = tokenizer(tokens, is_split_into_words=True, truncation=True, padding=False)\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=0)  # list of word_id per tokenized token\n",
    "\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(label_to_id[\"O\"])\n",
    "        else:\n",
    "            # if new word, use B- or I- as provided (we have token-level BIO already)\n",
    "            lab = labels[word_idx]\n",
    "            aligned_labels.append(label_to_id[lab])\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Test alignment on a single example (debug)\n",
    "print(\"Example tokens:\", dataset[\"train\"][0][\"tokens\"])\n",
    "print(\"Example labels:\", dataset[\"train\"][0][\"labels\"])\n",
    "print(tokenize_and_align_labels(dataset[\"train\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63508b55-d975-4b87-9ad2-d82da8e01a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 6\n",
    "def hf_tokenize_align(batch):\n",
    "    tokenized = tokenizer(batch[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    all_labels = []\n",
    "    for i, label_seq in enumerate(batch[\"labels\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned.append(-100)                         # ignore in loss\n",
    "            else:\n",
    "                aligned.append(label_to_id[label_seq[word_idx]])\n",
    "        all_labels.append(aligned)\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(hf_tokenize_align, batched=True, remove_columns=[\"tokens\",\"labels\"])\n",
    "\n",
    "tokenized_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fc51b-4994-4ca4-9aaf-a6a0c7626fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# very small / fast options for debugging:\n",
    "# \"prajjwal1/bert-tiny\"  (tiny BERT)\n",
    "# \"sshleifer/tiny-distilbert-base-cased\"\n",
    "model_checkpoint = \"sshleifer/tiny-distilbert-base-cased\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "print(\"Loaded model:\", model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531963e-76a9-415a-ad99-92b64299cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3afa6b-59c7-44b4-80ea-cc8e21e0d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ping huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a54a34-26ef-4178-a278-b3dcba461c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 8\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Convert predicted ids to label strings and compute seqeval metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # predictions: [batch_size, seq_len, num_labels] -> argmax\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        lab = labels[i]\n",
    "        pred = preds[i]\n",
    "        # iterate tokens and skip label == -100 if present (but we didn't set -100 earlier)\n",
    "        seq_true = []\n",
    "        seq_pred = []\n",
    "        for j, lab_id in enumerate(lab):\n",
    "            # if label is -100 (ignored), skip. In our pipeline, we used label_to_id only.\n",
    "            if lab_id == -100:\n",
    "                continue\n",
    "            seq_true.append(id_to_label[int(lab_id)])\n",
    "            seq_pred.append(id_to_label[int(pred[j])])\n",
    "        true_labels.append(seq_true)\n",
    "        true_preds.append(seq_pred)\n",
    "\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    # results is dict with per-entity results and overall\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 0.0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddeb453-f3c4-40ac-9d3f-552f32bd6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 9\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/finetuned-bert-job-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    push_to_hub=False,\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c32f5-cc54-47bb-b6c2-23fc366dfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 10\n",
    "metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(\"models/finetuned-bert-job-ner\")\n",
    "tokenizer.save_pretrained(\"models/finetuned-bert-job-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e7bd9-49b5-4346-b9af-27e1ca48d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook cell 11\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp_pipeline = pipeline(\"token-classification\", model=\"models/finetuned-bert-job-ner\", tokenizer=\"models/finetuned-bert-job-ner\", aggregation_strategy=\"none\")\n",
    "\n",
    "def predict_on_raw_text(raw_tokens):\n",
    "    # raw_tokens: list of tokens (same style as dataset tokens)\n",
    "    # For simplicity we join and set is_split_into_words=True when tokenizing\n",
    "    encoding = tokenizer(raw_tokens, is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**{k:v.to(model.device) for k,v in encoding.items()})\n",
    "    logits = outputs.logits.cpu().numpy()\n",
    "    preds = np.argmax(logits, axis=-1)[0]\n",
    "\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "    bio_preds = []\n",
    "    for idx, wid in enumerate(word_ids):\n",
    "        if wid is None:\n",
    "            bio_preds.append((\"\",\"O\"))\n",
    "        else:\n",
    "            label = id_to_label[int(preds[idx])]\n",
    "            bio_preds.append((raw_tokens[wid], label))\n",
    "    # compress to one label per original token (skip duplicates from wordpiece)\n",
    "    final = []\n",
    "    last_wid = None\n",
    "    for (tok, lab), pos in zip(bio_preds, range(len(bio_preds))):\n",
    "        # this simplifies â€” better to iterate word ids and pick first occurrence per word\n",
    "        pass\n",
    "\n",
    "# A simpler approach: use our tokenized pipeline above per sentence via the Trainer's predict function:\n",
    "def predict_sentence(tokens_list):\n",
    "    # tokens_list: list of tokens\n",
    "    enc = tokenizer(tokens_list, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(**{k:v.to(model.device) for k,v in enc.items()})\n",
    "    logits = out.logits.detach().cpu().numpy()\n",
    "    pred_ids = np.argmax(logits, axis=2)[0]\n",
    "    word_ids = enc.word_ids(batch_index=0)\n",
    "    pred_labels = []\n",
    "    last_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        if word_idx != last_word_idx:\n",
    "            pred_labels.append(id_to_label[int(pred_ids[idx])])\n",
    "            last_word_idx = word_idx\n",
    "    # pred_labels now aligned 1:1 with tokens_list\n",
    "    return list(zip(tokens_list, pred_labels))\n",
    "\n",
    "# Example usage:\n",
    "# tokens = dataset[\"test\"][0][\"tokens\"]\n",
    "# print(predict_sentence(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcc4b2-305e-46f0-9fd5-ed99183e24ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
